{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, requests as rq\n",
    "import json, re, time, datetime\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, SimpleRNN, Conv1D, Flatten\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (mean_absolute_error,\n",
    "                             mean_absolute_percentage_error,\n",
    "                             mean_squared_error, r2_score)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(tickers = []):\n",
    "    all_tickers_df = pd.DataFrame()\n",
    "    for i in tickers:\n",
    "        headers = {\"Accept\":\"text/html\", \"Accept-Language\":\"en-US\", \"Referer\":\"https://www.nasdaq.com/\", \"User-Agent\":\"Chrome/64.0.3282.119\"}\n",
    "        resp = rq.get(f'https://api.nasdaq.com/api/quote/{i}/chart?assetclass=stocks&fromdate=2013-02-02&todate={datetime.datetime.now().strftime(\"%Y-%m-%d\")}', headers=headers, verify=True)\n",
    "        if resp.status_code == 200:\n",
    "            try:\n",
    "                parsed_resp = json.loads(re.search('\\[.*\\]', resp.text).group())\n",
    "                current_ticker_df = pd.DataFrame([parsed_resp[k]['z'] for k in range(len(parsed_resp))])\n",
    "                current_ticker_df['ticker'] = i\n",
    "                for col_name in ['high','low','open','close','volume','value']:\n",
    "                    current_ticker_df[col_name] = pd.to_numeric(current_ticker_df[col_name].str.replace(',',''))\n",
    "                current_ticker_df['dateTime'] = pd.to_datetime(current_ticker_df['dateTime'])\n",
    "                all_tickers_df = pd.concat([all_tickers_df, current_ticker_df])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        else:\n",
    "            print(f'ERROR: smth is wrong with {i}')\n",
    "    all_tickers_df.to_parquet(f'data.parquet.gz', compression='gzip')\n",
    "    print(f'Info about {len(tickers)} tickers successfully loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    __minimum = 0\n",
    "    __maximum = 0\n",
    "\n",
    "    def __init__(self, params = None):\n",
    "        if params is not None:\n",
    "            self.__minimum, self.__maximum = params[0], params[1]\n",
    "\n",
    "    def fit(self, array):\n",
    "        self.__minimum = np.min(array)\n",
    "        self.__maximum = np.max(array)\n",
    "        return self\n",
    "\n",
    "    def scale(self, array) -> np.array:\n",
    "        return (array-self.__minimum)/(self.__maximum - self.__minimum)\n",
    "\n",
    "    def unscale(self, array) -> np.array:\n",
    "        return array*(self.__maximum - self.__minimum) + self.__minimum\n",
    "\n",
    "    def params(self) -> list:\n",
    "        return self.__minimum, self.__maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ticker = 'AAPL', column = 'close', length = 15, step = 1, start_date = '01-01-2013', split_date = '01-01-2022', end_date = '01-01-2023', flatten = False, validate = False):\n",
    "\n",
    "    data = pd.read_parquet('data.parquet.gz')\n",
    "    data = data[(data.dateTime >= start_date) & (data.ticker == ticker)]\n",
    "\n",
    "    train_data = data[(data.dateTime < split_date)][column].values.reshape(-1, 1)\n",
    "    test_data =  data[(data.dateTime >= split_date) & (data.dateTime < end_date)][column].values.reshape(-1, 1)\n",
    "\n",
    "    train_set = [train_data[j:j+length,0] for j in range(0, len(train_data)-length, step)]\n",
    "    test_set =  [test_data[j:j+length,0] for j in range(0, len(test_data)-length, step)]\n",
    "\n",
    "    scaler = Scaler().fit(train_set)\n",
    "    scaled_train_set = scaler.scale(train_set)\n",
    "    scaled_test_set = scaler.scale(test_set)\n",
    "\n",
    "    scaled_x_train, scaled_x_test = scaled_train_set[:,:length-1], scaled_test_set[:,:length-1]\n",
    "    scaled_y_train, scaled_y_test = scaled_train_set[:,length-1],  scaled_test_set[:,length-1]\n",
    "\n",
    "    if not flatten:\n",
    "        scaled_x_train = np.reshape(scaled_x_train, (scaled_x_train.shape[0], length - 1,1))\n",
    "        scaled_x_test = np.reshape(scaled_x_test, (scaled_x_test.shape[0], length - 1,1))\n",
    "\n",
    "    if validate:\n",
    "        val_data = data[(data.dateTime >= end_date) & (data.ticker == ticker)][column].values.reshape(-1, 1)\n",
    "        val_set = [val_data[j:j+length,0] for j in range(0, len(val_data)-length, step)]\n",
    "        scaled_val_set = scaler.scale(val_set)\n",
    "        scaled_x_val, scaled_y_val = scaled_val_set[:,:length-1], scaled_val_set[:,length-1]\n",
    "        return scaled_x_train, scaled_x_test, scaled_x_val, scaled_y_train, scaled_y_test, scaled_y_val, scaler\n",
    "    else:\n",
    "        return scaled_x_train, scaled_x_test, scaled_y_train, scaled_y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topologies(lag = 30):\n",
    "    \n",
    "    neurons_per_layer = lag-1\n",
    "    input_shape = (neurons_per_layer, 1)\n",
    "\n",
    "    return {'CNN + LSTM': [Conv1D(filters = 128, kernel_size = 3, activation = 'relu', input_shape = input_shape),\n",
    "                                LSTM(units = neurons_per_layer),\n",
    "                                Dense(units = 1)],\n",
    "            'LSTM x3': [LSTM(units = neurons_per_layer,\n",
    "                            return_sequences = True,\n",
    "                            input_shape = input_shape),\n",
    "                        LSTM(units = neurons_per_layer,\n",
    "                            return_sequences = True),\n",
    "                        LSTM(units = neurons_per_layer),\n",
    "                        Dense(units = 1)],\n",
    "            'LSTM x2': [LSTM(units = neurons_per_layer,\n",
    "                            return_sequences = True,\n",
    "                            input_shape = input_shape),\n",
    "                        LSTM(units = neurons_per_layer),\n",
    "                        Dense(units = 1)],\n",
    "            'LSTM x1': [LSTM(units = neurons_per_layer,\n",
    "                            input_shape = input_shape),\n",
    "                        Dense(units = 1)],\n",
    "            'CNN + GRU': [Conv1D(filters = 128, kernel_size = 3, activation = 'relu', input_shape = input_shape),\n",
    "                        GRU(units = neurons_per_layer),\n",
    "                        Dense(units = 1)],\n",
    "            'GRU x3' : [GRU(units = neurons_per_layer,\n",
    "                            return_sequences = True,\n",
    "                            input_shape = input_shape),\n",
    "                        GRU(units = neurons_per_layer,\n",
    "                            return_sequences = True),\n",
    "                        GRU(units = neurons_per_layer),\n",
    "                        Dense(units = 1)],\n",
    "            'GRU x2' : [GRU(units = neurons_per_layer,\n",
    "                            return_sequences = True,\n",
    "                            input_shape = input_shape),\n",
    "                        GRU(units = neurons_per_layer),\n",
    "                        Dense(units = 1)],\n",
    "            'GRU x1' : [GRU(units = neurons_per_layer,\n",
    "                            input_shape = input_shape),\n",
    "                        Dense(units = 1)],\n",
    "            'CNN + SimpleRNN': [Conv1D(filters = 128, kernel_size = 3, activation = 'relu', input_shape = input_shape),\n",
    "                                SimpleRNN(units = neurons_per_layer),\n",
    "                                Dense(units = 1)],\n",
    "            'SimpleRNN x3':[SimpleRNN(units = neurons_per_layer,\n",
    "                                    return_sequences = True,\n",
    "                                    input_shape = input_shape),\n",
    "                            SimpleRNN(units = neurons_per_layer,\n",
    "                                    return_sequences = True),\n",
    "                            SimpleRNN(units = neurons_per_layer),\n",
    "                            Dense(units = 1)],\n",
    "            'SimpleRNN x2':[SimpleRNN(units = neurons_per_layer,\n",
    "                                    return_sequences = True,\n",
    "                                    input_shape = input_shape),\n",
    "                            SimpleRNN(units = neurons_per_layer),\n",
    "                            Dense(units = 1)],\n",
    "            'SimpleRNN x1':[SimpleRNN(units = neurons_per_layer,\n",
    "                                    input_shape = input_shape),\n",
    "                            Dense(units = 1)],\n",
    "            'CNN': [Conv1D(filters = 32, kernel_size = 5, input_shape = input_shape, activation = 'relu'),\n",
    "                    Flatten(),\n",
    "                    Dense(units = 1)],\n",
    "            'MLP(3)': [Dense(units = neurons_per_layer, input_shape = (neurons_per_layer,)),\n",
    "                    Dense(units = neurons_per_layer*2),\n",
    "                    Dense(units = neurons_per_layer),\n",
    "                    Dense(units = 1)],\n",
    "            'MLP(2)': [Dense(units = neurons_per_layer, input_shape = (neurons_per_layer,)),\n",
    "                    Dense(units = neurons_per_layer),\n",
    "                    Dense(units = 1)],\n",
    "            'MLP(1)': [Dense(units = neurons_per_layer, input_shape = (neurons_per_layer,)),\n",
    "                    Dense(units = 1)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ml_models(x_train, x_test, y_train, y_test) -> list:\n",
    "\n",
    "    statistics = []\n",
    "    models = {  'LR' :  LinearRegression(),\n",
    "                'DTR' : DecisionTreeRegressor(min_samples_leaf=5),\n",
    "                'RFR' : RandomForestRegressor(),\n",
    "                'GBR' : GradientBoostingRegressor(),\n",
    "                'SVR' : SVR(kernel='linear', epsilon=1e-3),\n",
    "                'CBR' : CatBoostRegressor(loss_function='MAPE'),\n",
    "                'XGBR': XGBRegressor(objective='reg:squarederror'),\n",
    "                'XGBRFR': XGBRFRegressor(objective = 'reg:squarederror')}\n",
    "    \n",
    "    for model_decription, model in models.items():\n",
    "        start_time = time.time()\n",
    "        model.fit(x_train, y_train)\n",
    "        utilized = time.time() - start_time\n",
    "        preds = model.predict(x_test)\n",
    "        statistics.append({'time':utilized, 'mse': mean_squared_error(y_test, preds),'mae': mean_absolute_error(y_test, preds),'mape': mean_absolute_percentage_error(y_test, preds),'model': model_decription})\n",
    "    return statistics\n",
    "\n",
    "def evaluate_autoregressive_models(x_train, x_test, y_train, y_test) -> list:\n",
    "\n",
    "    statistics = []\n",
    "    p, q = 2, 1\n",
    "    for d in range(2):\n",
    "        mse, mae, mape = 0, 0, 0\n",
    "        for i in range(int(len(y_test)/10)):\n",
    "            start_time = time.time()\n",
    "            model = ARIMA(x_test[i], order = (p, d, q), enforce_stationarity=True)\n",
    "            forecast = model.fit().forecast(steps = 1)\n",
    "            utilized = time.time() - start_time\n",
    "            mse += (forecast - y_test[i]) **2\n",
    "            mae += abs(forecast - y_test[i])\n",
    "            mape += abs(forecast - y_test[i])/y_test[i]\n",
    "        statistics.append({'time' : utilized, 'mse': (mse / int(len(y_test) / 10))[0],'mae': (mae / int(len(y_test) / 10))[0], 'model': f'ARIMA({p},{d},{q})' if d > 0 else f'ARMA({p},{q})'})\n",
    "    return statistics\n",
    "\n",
    "def evaluate_neural_networks(topologies_dict: dict, x_train, x_test, y_train, y_test, epochs = 5, batch_size = 1000) -> list:\n",
    "    statistics = []\n",
    "    for topology_description, topology in topologies_dict.items():\n",
    "        current_topology_model = Sequential(topology)\n",
    "        current_topology_model.compile(optimizer = 'Adamax', loss = 'mse', metrics = ['mae', 'mape'])\n",
    "        start_time = time.time()\n",
    "        current_topology_model.fit(x = x_train, y = y_train, epochs = epochs, batch_size = batch_size, verbose = 0)\n",
    "        utilized_time = time.time() - start_time\n",
    "        mse, mae, mape = current_topology_model.evaluate(x_test, y_test, verbose = 0)\n",
    "        statistics.append({'time': utilized_time, 'mse': mse, 'mae': mae, 'mape': mape, 'model': topology_description})\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_topologies(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Использовались данные о стоимости акций 14 крупнейших компаний, представленных на бирже Nasdaq.  \n",
    "Подготовка данных включает нарезку данных на отрезки определенной длины с помощью скользящего окна и MinMax нормализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info about 14 tickers successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "update_data(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models benchmarking\n",
    "Для определения наиболее подходящей модели, было проведено тестирование:\n",
    "данные усреднялись по 13 акциям, метрики MAE/MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: GRU x2\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in tickers:\n",
    "\n",
    "    x_train, x_test, y_train, y_test, scaler = get_data(i, flatten = True)\n",
    "    results += evaluate_autoregressive_models(x_train, x_test, y_train, y_test)\n",
    "    results += evaluate_ml_models(x_train, x_test, y_train, y_test)\n",
    "\n",
    "    x_train, x_test, y_train, y_test, scaler = get_data(i)\n",
    "    results += evaluate_neural_networks(get_topologies(15), x_train, x_test, y_train, y_test)\n",
    "    \n",
    "results = pd.DataFrame(results).groupby(by='model').mean().sort_values(by='mape')\n",
    "print(f'Best model: {results.index[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "Для подбора оптимального набора гиперпараметров использовал gridsearch, всего 1008 комбинаций.\n",
    "6 вариантов числа эпох, 6 вариантов размера батча, 7 оптимизаторов и 4 функции ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams combo: [5 32 'Nadam' 'MSE']\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "x_train,x_test,y_train,y_test,scaler = get_data()\n",
    "\n",
    "for ep in [2,5,10,15,25,50]:\n",
    "    for bs in [32,64,128,256,512,1024]:\n",
    "        for opt in ['adam', 'adamax', 'adadelta', 'adagrad', 'nadam', 'rmsprop', 'sgd']:\n",
    "            for loss in ['mse', 'mae', 'mape', 'msle']:\n",
    "                current_topology_model = Sequential(get_topologies(15)['GRU x_test'])\n",
    "                current_topology_model.compile(optimizer = opt, loss = loss)\n",
    "                current_topology_model.fit(x = x_train, y = y_train, epochs = ep, batch_size = bs, verbose = 0)\n",
    "                preds = current_topology_model.predict(x_test, verbose=1).reshape(-1)\n",
    "                results +=[{'mse': mean_squared_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                            'mae': mean_absolute_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                            'mape': mean_absolute_percentage_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                            'r2':  r2_score(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                            'epochs':ep,\n",
    "                            'batch_size':bs,\n",
    "                            'optimizer':opt,\n",
    "                            'loss':loss}]\n",
    "\n",
    "results = pd.DataFrame(results).sort_values(by = 'mape')\n",
    "print(f'Best hyperparams combo: {results.iloc[0,:].values[-4:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model training\n",
    "Обучал модель с учетом предыдущих результатов тестов.  \n",
    "Топология: входной слой на 14 нейронов, два слоя GRU по 14 нейронов, выходной слой на 1 нейрон.  \n",
    "Гиперпараметры: 5 эпох, размер батча 32, оптимизатор Nadam, функция ошибки MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Sequential(get_topologies(15)['GRU x2'])\n",
    "final_model.compile(optimizer = 'Nadam', loss = 'mse')\n",
    "for i in tickers:\n",
    "    try:\n",
    "\n",
    "        x_train, x_test, y_train, y_test, scaler = get_data(ticker = i)\n",
    "        final_model.fit(x = x_train, y = y_train, epochs = 5, batch_size = 32, verbose = 0)\n",
    "    except:\n",
    "        pass\n",
    "save_model(final_model, 'trained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Модель тестировалась на данных для обучения, данных для тестирования и данных о стоимости акций 50 других компаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данных для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse     15.328833\n",
       "mae      2.089029\n",
       "mape     0.019872\n",
       "r2       0.997816\n",
       "dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i in tickers: \n",
    "    x_train, x_test, y_train, y_test, scaler = get_data(ticker = i)\n",
    "    preds = final_model.predict(x_train, verbose=0)\n",
    "    results +=[{'mse': mean_squared_error(scaler.unscale(y_train), scaler.unscale(preds)),\n",
    "                'mae': mean_absolute_error(scaler.unscale(y_train), scaler.unscale(preds)),\n",
    "                'mape': mean_absolute_percentage_error(scaler.unscale(y_train), scaler.unscale(preds)),\n",
    "                'r2':  r2_score(scaler.unscale(y_train), scaler.unscale(preds))}]\n",
    "pd.DataFrame(results).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данных для тестирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse     30.217507\n",
       "mae      3.195481\n",
       "mape     0.018640\n",
       "r2       0.937693\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i in tickers: \n",
    "    x_train, x_test, y_train, y_test, scaler = get_data(ticker = i)\n",
    "    preds = final_model.predict(x_test, verbose=0)\n",
    "    results +=[{'mse': mean_squared_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                'mae': mean_absolute_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                'mape': mean_absolute_percentage_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                'r2':  r2_score(scaler.unscale(y_test), scaler.unscale(preds))}]\n",
    "pd.DataFrame(results).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данных о стоимости акций других компаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info about 50 tickers successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "update_data(json.loads(open('ticker_test.json').read()))\n",
    "tickers_for_test = json.loads(open('ticker_test.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse     27.044111\n",
       "mae      3.174334\n",
       "mape     0.023164\n",
       "r2       0.918939\n",
       "dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i in tickers_for_test: \n",
    "    x_train, x_test, y_train, y_test, scaler = get_data(ticker = i)\n",
    "    preds = final_model.predict(x_test, verbose=0)\n",
    "    results +=[{'mse': mean_squared_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                'mae': mean_absolute_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                'mape': mean_absolute_percentage_error(scaler.unscale(y_test), scaler.unscale(preds)),\n",
    "                'r2':  r2_score(scaler.unscale(y_test), scaler.unscale(preds))}]\n",
    "pd.DataFrame(results).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
